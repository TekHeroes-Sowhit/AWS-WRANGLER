{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awswrangler has 3 different copy modes to store Parquet Datasets on Amazon S3.\n",
    "\n",
    "# append (Default)\n",
    "\n",
    "# Only adds new files without any delete.\n",
    "\n",
    "# overwrite\n",
    "\n",
    "# Deletes everything in the target directory and then add new files.\n",
    "\n",
    "# overwrite_partitions (Partition Upsert)\n",
    "\n",
    "# Only deletes the paths of partitions that should be updated and then writes the new partitions files. It’s like a “partition Upsert”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "bucket = getpass.getpass()\n",
    "path1 = f\"s3://{bucket}/dataset1/\"\n",
    "path2 = f\"s3://{bucket}/dataset2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"], \"date\": [date(2020, 1, 1), date(2020, 1, 2)]})\n",
    "wr.s3.to_parquet(df=df, path=path1, dataset=True, mode=\"overwrite\", partition_cols=[\"date\"])\n",
    "wr.s3.read_parquet(path1, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [2, 3], \"value\": [\"xoo\", \"bar\"], \"date\": [date(2020, 1, 2), date(2020, 1, 3)]})\n",
    "dataset2_files = wr.s3.to_parquet(df=df, path=path2, dataset=True, mode=\"overwrite\", partition_cols=[\"date\"])[\"paths\"]\n",
    "wr.s3.read_parquet(path2, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.merge_datasets(source_path=path2, target_path=path1, mode=\"append\")\n",
    "wr.s3.read_parquet(path1, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.merge_datasets(source_path=path2, target_path=path1, mode=\"overwrite_partitions\")\n",
    "wr.s3.read_parquet(path1, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.delete_objects(path1)\n",
    "wr.s3.delete_objects(path2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
