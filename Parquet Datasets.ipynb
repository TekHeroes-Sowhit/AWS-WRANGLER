{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "bucket = getpass.getpass()\n",
    "path = f\"s3://{bucket}/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"], \"date\": [date(2020, 1, 1), date(2020, 1, 2)]})\n",
    "wr.s3.to_parquet(df=df, path=path, dataset=True, mode=\"overwrite\")\n",
    "wr.s3.read_parquet(path, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [3], \"value\": [\"bar\"], \"date\": [date(2020, 1, 3)]})\n",
    "wr.s3.to_parquet(df=df, path=path, dataset=True, mode=\"append\")\n",
    "wr.s3.read_parquet(path, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.to_parquet(df=df, path=path, dataset=True, mode=\"overwrite\")\n",
    "wr.s3.read_parquet(path, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Partitioned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"], \"date\": [date(2020, 1, 1), date(2020, 1, 2)]})\n",
    "wr.s3.to_parquet(df=df, path=path, dataset=True, mode=\"overwrite\", partition_cols=[\"date\"])\n",
    "wr.s3.read_parquet(path, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upserting partitions (overwrite_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [2, 3], \"value\": [\"xoo\", \"bar\"], \"date\": [date(2020, 1, 2), date(2020, 1, 3)]})\n",
    "wr.s3.to_parquet(df=df, path=path, dataset=True, mode=\"overwrite_partitions\", partition_cols=[\"date\"])\n",
    "wr.s3.read_parquet(path, dataset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS - Glue/Athena integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"], \"date\": [date(2020, 1, 1), date(2020, 1, 2)]})\n",
    "wr.s3.to_parquet(df=df, path=path, dataset=True, mode=\"overwrite\", database=\"aws_sdk_pandas\", table=\"my_table\")\n",
    "wr.athena.read_sql_query(\"SELECT * FROM my_table\", database=\"aws_sdk_pandas\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
