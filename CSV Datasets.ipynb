{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# awswrangler has 3 different write modes to store CSV Datasets on Amazon S3.\n",
    "\n",
    "# append (Default)\n",
    "\n",
    "# Only adds new files without any delete.\n",
    "\n",
    "# overwrite\n",
    "\n",
    "# Deletes everything in the target directory and then add new files.\n",
    "\n",
    "# overwrite_partitions (Partition Upsert)\n",
    "\n",
    "# Only deletes the paths of partitions that should be updated and then writes the new partitions files. It’s like a “partition Upsert”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "import pandas as pd\n",
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your bucket name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "bucket = getpass.getpass()\n",
    "path = f\"s3://{bucket}/dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking/Creating Glue Catalog Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"awswrangler_test\" not in wr.catalog.databases().values:\n",
    "    wr.catalog.create_database(\"awswrangler_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"], \"date\": [date(2020, 1, 1), date(2020, 1, 2)]})\n",
    "\n",
    "wr.s3.to_csv(\n",
    "    df=df, path=path, index=False, dataset=True, mode=\"overwrite\", database=\"awswrangler_test\", table=\"csv_dataset\"\n",
    ")\n",
    "\n",
    "wr.athena.read_sql_table(database=\"awswrangler_test\", table=\"csv_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [3], \"value\": [\"bar\"], \"date\": [date(2020, 1, 3)]})\n",
    "\n",
    "wr.s3.to_csv(\n",
    "    df=df, path=path, index=False, dataset=True, mode=\"append\", database=\"awswrangler_test\", table=\"csv_dataset\"\n",
    ")\n",
    "\n",
    "wr.athena.read_sql_table(database=\"awswrangler_test\", table=\"csv_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr.s3.to_csv(\n",
    "    df=df, path=path, index=False, dataset=True, mode=\"overwrite\", database=\"awswrangler_test\", table=\"csv_dataset\"\n",
    ")\n",
    "\n",
    "wr.athena.read_sql_table(database=\"awswrangler_test\", table=\"csv_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Partitioned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"], \"date\": [date(2020, 1, 1), date(2020, 1, 2)]})\n",
    "\n",
    "wr.s3.to_csv(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    index=False,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite\",\n",
    "    database=\"awswrangler_test\",\n",
    "    table=\"csv_dataset\",\n",
    "    partition_cols=[\"date\"],\n",
    ")\n",
    "\n",
    "wr.athena.read_sql_table(database=\"awswrangler_test\", table=\"csv_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upserting partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [2, 3], \"value\": [\"xoo\", \"bar\"], \"date\": [date(2020, 1, 2), date(2020, 1, 3)]})\n",
    "\n",
    "wr.s3.to_csv(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    index=False,\n",
    "    dataset=True,\n",
    "    mode=\"overwrite_partitions\",\n",
    "    database=\"awswrangler_test\",\n",
    "    table=\"csv_dataset\",\n",
    "    partition_cols=[\"date\"],\n",
    ")\n",
    "\n",
    "wr.athena.read_sql_table(database=\"awswrangler_test\", table=\"csv_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS - Glue/Athena integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [1, 2], \"value\": [\"foo\", \"boo\"], \"date\": [date(2020, 1, 1), date(2020, 1, 2)]})\n",
    "\n",
    "wr.s3.to_csv(\n",
    "    df=df,\n",
    "    path=path,\n",
    "    dataset=True,\n",
    "    index=False,\n",
    "    mode=\"overwrite\",\n",
    "    database=\"aws_sdk_pandas\",\n",
    "    table=\"my_table\",\n",
    "    compression=\"gzip\",\n",
    ")\n",
    "\n",
    "wr.athena.read_sql_query(\"SELECT * FROM my_table\", database=\"aws_sdk_pandas\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
